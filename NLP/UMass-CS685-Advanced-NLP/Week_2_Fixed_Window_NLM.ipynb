{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week-2-Fixed-Window-NLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+h/wfkLM4+WfeQ6zBWtXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u6yuvi/Courses/blob/main/NLP/UMass-CS685-Advanced-NLP/Week_2_Fixed_Window_NLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "Given the first three words(window_size) of each sentence, we will try to predict the third word using a fixed window NLM.\n",
        "\n",
        "Simplification:\n",
        "1. All text are of equal length."
      ],
      "metadata": {
        "id": "LKxoVpZioApu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9nC_hRdlptg"
      },
      "outputs": [],
      "source": [
        "sentences = [\"This a Week-2 Course\",\"Learning Fixed Window NLM\",\"Train a NLM Model\",\"One Last Example Set\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "inputs = []\n",
        "for sent in sentences:\n",
        "  sent_indexes = []\n",
        "  for word in sent.split(\" \"):\n",
        "    if word not in vocab:\n",
        "        vocab[word] = len(vocab)\n",
        "    sent_indexes.append(vocab[word])\n",
        "  inputs.append(sent_indexes)"
      ],
      "metadata": {
        "id": "wX74aJ8-l3WO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab),print(len(vocab))\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofVY8xL7mX7U",
        "outputId": "8c387efd-41fb-44f4-a6df-a17f2c59eb5e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'This': 0, 'a': 1, 'Week-2': 2, 'Course': 3, 'Learning': 4, 'Fixed': 5, 'Window': 6, 'NLM': 7, 'Train': 8, 'Model': 9, 'One': 10, 'Last': 11, 'Example': 12, 'Set': 13}\n",
            "14\n",
            "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 1, 7, 9], [10, 11, 12, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "prefixes = torch.LongTensor([i[:-1] for i in inputs])\n",
        "labels = torch.LongTensor([i[-1] for i in inputs])\n",
        "print(prefixes,labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPmwZLYJmorj",
        "outputId": "e6aa27e1-3232-4c4b-e72a-bcc4ee183786"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2],\n",
            "        [ 4,  5,  6],\n",
            "        [ 8,  1,  7],\n",
            "        [10, 11, 12]]) tensor([ 3,  7,  9, 13])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "9_y5D3EGuXog"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLM(nn.Module):\n",
        "  '''\n",
        "  Two things to implement\n",
        "  1. init function to initialize things\n",
        "  2. Forward Pass\n",
        "  '''\n",
        "  def __init__(self,d_embeddings,d_hidden,window_size,len_vocab) -> None:\n",
        "    super(NLM,self).__init__()\n",
        "\n",
        "    self.d_embed = d_embeddings \n",
        "    self.embeddings = nn.Embedding(len_vocab,d_embeddings)\n",
        "    # concatenated embeddings > hidden\n",
        "    self.W_hid = nn.Linear(d_embeddings*window_size,d_hidden)\n",
        "    # hidden > output probability distribution over vocab\n",
        "    self.W_out = nn.Linear(d_hidden,len_vocab)\n",
        "\n",
        "  def forward(self,input): #each input will be a batch of prefixes\n",
        "    batch_size , window_size = input.size()\n",
        "    embs = self.embeddings(input) #4x3x5 [batch_size,window_size,d_embeddings]\n",
        "    #print(embs.shape)\n",
        "\n",
        "    concat_embs = embs.view(batch_size,window_size*self.d_embed)\n",
        "    #print(concat_embs.shape)\n",
        "    hiddens = self.W_hid(concat_embs)\n",
        "\n",
        "    outs = self.W_out(hiddens)\n",
        "    #print(outs.shape)\n",
        "\n",
        "    return outs\n",
        "\n",
        "network = NLM(d_embeddings=5,d_hidden=10,window_size = 3, len_vocab=len(vocab))\n",
        "network(prefixes)\n",
        "\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "optimiser  = torch.optim.SGD(params = network.parameters(),lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Nl9GE-zLuaU4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epochs):\n",
        "  logits = network(prefixes)\n",
        "  loss = loss_fn(logits,labels)\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "  optimiser.zero_grad()\n",
        "  print(f'Loss after Epoch-{i} {loss:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTlc_kNk_0i1",
        "outputId": "9095e559-fad4-48a0-f21f-17210407bc42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after Epoch-0 2.59\n",
            "Loss after Epoch-1 2.57\n",
            "Loss after Epoch-2 2.55\n",
            "Loss after Epoch-3 2.53\n",
            "Loss after Epoch-4 2.50\n",
            "Loss after Epoch-5 2.48\n",
            "Loss after Epoch-6 2.46\n",
            "Loss after Epoch-7 2.44\n",
            "Loss after Epoch-8 2.42\n",
            "Loss after Epoch-9 2.40\n",
            "Loss after Epoch-10 2.38\n",
            "Loss after Epoch-11 2.36\n",
            "Loss after Epoch-12 2.34\n",
            "Loss after Epoch-13 2.32\n",
            "Loss after Epoch-14 2.30\n",
            "Loss after Epoch-15 2.28\n",
            "Loss after Epoch-16 2.26\n",
            "Loss after Epoch-17 2.24\n",
            "Loss after Epoch-18 2.22\n",
            "Loss after Epoch-19 2.20\n",
            "Loss after Epoch-20 2.18\n",
            "Loss after Epoch-21 2.16\n",
            "Loss after Epoch-22 2.14\n",
            "Loss after Epoch-23 2.12\n",
            "Loss after Epoch-24 2.10\n",
            "Loss after Epoch-25 2.08\n",
            "Loss after Epoch-26 2.06\n",
            "Loss after Epoch-27 2.04\n",
            "Loss after Epoch-28 2.02\n",
            "Loss after Epoch-29 2.00\n",
            "Loss after Epoch-30 1.98\n",
            "Loss after Epoch-31 1.95\n",
            "Loss after Epoch-32 1.93\n",
            "Loss after Epoch-33 1.91\n",
            "Loss after Epoch-34 1.89\n",
            "Loss after Epoch-35 1.87\n",
            "Loss after Epoch-36 1.85\n",
            "Loss after Epoch-37 1.83\n",
            "Loss after Epoch-38 1.81\n",
            "Loss after Epoch-39 1.79\n",
            "Loss after Epoch-40 1.77\n",
            "Loss after Epoch-41 1.75\n",
            "Loss after Epoch-42 1.73\n",
            "Loss after Epoch-43 1.71\n",
            "Loss after Epoch-44 1.69\n",
            "Loss after Epoch-45 1.67\n",
            "Loss after Epoch-46 1.65\n",
            "Loss after Epoch-47 1.63\n",
            "Loss after Epoch-48 1.61\n",
            "Loss after Epoch-49 1.59\n",
            "Loss after Epoch-50 1.57\n",
            "Loss after Epoch-51 1.55\n",
            "Loss after Epoch-52 1.53\n",
            "Loss after Epoch-53 1.51\n",
            "Loss after Epoch-54 1.49\n",
            "Loss after Epoch-55 1.47\n",
            "Loss after Epoch-56 1.45\n",
            "Loss after Epoch-57 1.43\n",
            "Loss after Epoch-58 1.41\n",
            "Loss after Epoch-59 1.39\n",
            "Loss after Epoch-60 1.37\n",
            "Loss after Epoch-61 1.35\n",
            "Loss after Epoch-62 1.33\n",
            "Loss after Epoch-63 1.31\n",
            "Loss after Epoch-64 1.30\n",
            "Loss after Epoch-65 1.28\n",
            "Loss after Epoch-66 1.26\n",
            "Loss after Epoch-67 1.24\n",
            "Loss after Epoch-68 1.22\n",
            "Loss after Epoch-69 1.20\n",
            "Loss after Epoch-70 1.18\n",
            "Loss after Epoch-71 1.16\n",
            "Loss after Epoch-72 1.15\n",
            "Loss after Epoch-73 1.13\n",
            "Loss after Epoch-74 1.11\n",
            "Loss after Epoch-75 1.09\n",
            "Loss after Epoch-76 1.08\n",
            "Loss after Epoch-77 1.06\n",
            "Loss after Epoch-78 1.04\n",
            "Loss after Epoch-79 1.02\n",
            "Loss after Epoch-80 1.01\n",
            "Loss after Epoch-81 0.99\n",
            "Loss after Epoch-82 0.97\n",
            "Loss after Epoch-83 0.96\n",
            "Loss after Epoch-84 0.94\n",
            "Loss after Epoch-85 0.93\n",
            "Loss after Epoch-86 0.91\n",
            "Loss after Epoch-87 0.89\n",
            "Loss after Epoch-88 0.88\n",
            "Loss after Epoch-89 0.86\n",
            "Loss after Epoch-90 0.85\n",
            "Loss after Epoch-91 0.83\n",
            "Loss after Epoch-92 0.82\n",
            "Loss after Epoch-93 0.81\n",
            "Loss after Epoch-94 0.79\n",
            "Loss after Epoch-95 0.78\n",
            "Loss after Epoch-96 0.76\n",
            "Loss after Epoch-97 0.75\n",
            "Loss after Epoch-98 0.74\n",
            "Loss after Epoch-99 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Oohz8VaeI-U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our loss has dropped to close to 0.7.\n",
        "# but is it actually working? let's see\n",
        "\n",
        "# let's first define a reverse vocabulary mapping (idx>word type)\n",
        "rev_vocab = dict((idx, word) for (word, idx) in vocab.items())\n",
        "test_sample = prefixes[0].unsqueeze(0)\n",
        "logits = network(test_sample)\n",
        "probs = nn.functional.softmax(logits, dim=1).squeeze()\n",
        "argmax_idx = torch.argmax(probs).item()\n",
        "print('This a Week-2 Course\", the model predicts \"%s\" with %0.4f probability' % (rev_vocab[argmax_idx], probs[argmax_idx]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PANWzrGNI-Xi",
        "outputId": "e24091ab-efdf-4638-e05a-095b95faaaba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This a Week-2 Course\", the model predicts \"Course\" with 0.3999 probability\n"
          ]
        }
      ]
    }
  ]
}